{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-23T20:58:24.989296900Z",
     "start_time": "2024-06-23T20:58:22.962884200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from algorithm import (\n",
    "                        NoRegularizationTrainer,\n",
    "                        GradientTrainer,\n",
    "                        EGTrainer,\n",
    "                        CSVDataLoader,\n",
    "                        MultiTaskModel,\n",
    "                        MultiTaskDataset,\n",
    "                        mlt_train_test_split,\n",
    "                        # true_values_from_data_loader,\n",
    "                        unique_value_counts,\n",
    "                        Cindex,\n",
    "                        brier_score,\n",
    "                        )\n",
    "import easydict\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T20:58:25.004297200Z",
     "start_time": "2024-06-23T20:58:24.991296500Z"
    }
   },
   "id": "e797c6e4f1a84e30"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:01<05:14,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 0, Average Training Loss: 1.4687, Average Gradient Norm: 1.9357\n",
      "End of Epoch 0, Average Validation Loss: 1.4798\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:03<05:01,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 1, Average Training Loss: 1.1020, Average Gradient Norm: 1.6548\n",
      "End of Epoch 1, Average Validation Loss: 1.0778\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:04<04:45,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 2, Average Training Loss: 1.0756, Average Gradient Norm: 1.7185\n",
      "End of Epoch 2, Average Validation Loss: 1.1892\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:05<04:27,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 3, Average Training Loss: 1.0704, Average Gradient Norm: 1.7472\n",
      "End of Epoch 3, Average Validation Loss: 1.0063\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:07<04:29,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 4, Average Training Loss: 1.0382, Average Gradient Norm: 1.6607\n",
      "End of Epoch 4, Average Validation Loss: 1.3251\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 5, Average Training Loss: 1.0270, Average Gradient Norm: 1.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [00:08<04:29,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 5, Average Validation Loss: 1.1575\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [00:09<04:21,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 6, Average Training Loss: 1.0350, Average Gradient Norm: 1.7588\n",
      "End of Epoch 6, Average Validation Loss: 1.1998\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:11<04:22,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 7, Average Training Loss: 1.0111, Average Gradient Norm: 1.7326\n",
      "End of Epoch 7, Average Validation Loss: 1.1731\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:12<04:11,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 8, Average Training Loss: 1.0373, Average Gradient Norm: 1.7783\n",
      "End of Epoch 8, Average Validation Loss: 0.9998\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 9, Average Training Loss: 1.0058, Average Gradient Norm: 1.8109\n",
      "End of Epoch 9, Average Validation Loss: 1.1349\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [00:15<04:20,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 10, Average Training Loss: 1.0093, Average Gradient Norm: 1.7775\n",
      "End of Epoch 10, Average Validation Loss: 1.0200\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 11, Average Training Loss: 1.0021, Average Gradient Norm: 1.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [00:16<04:25,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 11, Average Validation Loss: 1.1520\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 13/200 [00:17<04:17,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 12, Average Training Loss: 0.9956, Average Gradient Norm: 1.7456\n",
      "End of Epoch 12, Average Validation Loss: 1.3522\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 13, Average Training Loss: 0.9993, Average Gradient Norm: 1.7341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/200 [00:19<04:19,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 13, Average Validation Loss: 1.1049\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15/200 [00:21<04:36,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 14, Average Training Loss: 0.9750, Average Gradient Norm: 1.6731\n",
      "End of Epoch 14, Average Validation Loss: 1.3212\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [00:22<04:32,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 15, Average Training Loss: 0.9517, Average Gradient Norm: 1.7033\n",
      "End of Epoch 15, Average Validation Loss: 1.1508\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 17/200 [00:23<04:15,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 16, Average Training Loss: 0.9337, Average Gradient Norm: 1.7474\n",
      "End of Epoch 16, Average Validation Loss: 1.0768\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 17, Average Training Loss: 0.9187, Average Gradient Norm: 1.7944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [00:25<04:20,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 17, Average Validation Loss: 1.1135\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [00:26<04:19,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 18, Average Training Loss: 0.9154, Average Gradient Norm: 1.9352\n",
      "End of Epoch 18, Average Validation Loss: 1.0689\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 19, Average Training Loss: 0.9093, Average Gradient Norm: 2.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [00:28<04:20,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 19, Average Validation Loss: 1.0925\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:29<04:22,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 20, Average Training Loss: 0.9077, Average Gradient Norm: 1.9360\n",
      "End of Epoch 20, Average Validation Loss: 1.1028\n",
      "Current Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 22/200 [00:31<04:15,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 21, Average Training Loss: 0.9026, Average Gradient Norm: 1.9630\n",
      "End of Epoch 21, Average Validation Loss: 1.0966\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 22, Average Training Loss: 0.8902, Average Gradient Norm: 2.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [00:32<04:22,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 22, Average Validation Loss: 1.0972\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 23, Average Training Loss: 0.8972, Average Gradient Norm: 1.9687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [00:34<04:24,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 23, Average Validation Loss: 1.0952\n",
      "Current Learning Rate: 0.000100\n",
      "Early stopping triggered after 24 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Dataset/processdata.csv', encoding='latin-1')\n",
    "date_columns = ['Date.of.Last.Contact', 'Date.of.Diagnostic']\n",
    "data[date_columns] = data[date_columns].apply(pd.to_datetime, errors='coerce')\n",
    "data['Survival_Time'] = (data['Date.of.Last.Contact'] - data['Date.of.Diagnostic']).dt.days\n",
    "data.loc[:, 'Survival_Time'] = data['Survival_Time'].replace({-1: 0})\n",
    "data['indicater'] = np.where(data['Date.of.Death'].isna(), 0, 1)\n",
    "\n",
    "\n",
    "columns_to_drop = ['Date.of.Death', 'Date.of.Last.Contact', 'Date.of.Diagnostic']\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "        range(len(data)), stratify=data['indicater'], random_state=1, test_size=0.25\n",
    "    )\n",
    "\n",
    "X = data.drop(['Survival_Time', 'indicater'], axis=1)\n",
    "time_all = data['Survival_Time'].values\n",
    "event_all = data['indicater'].values\n",
    "\n",
    "X_train = X.iloc[train_indices].copy()\n",
    "X_test = X.iloc[test_indices].copy()\n",
    "time_train = time_all[train_indices].copy()\n",
    "time_test = time_all[test_indices].copy()\n",
    "event_train = event_all[train_indices].copy()\n",
    "event_test = event_all[test_indices].copy()\n",
    "\n",
    "\n",
    "columns_to_one_hot = ['RCBP.Name', 'Raca.Color', 'State.Civil', 'Code.Profession', 'Name.Occupation',\n",
    "                              'Status.Address',\n",
    "                              'City.Address', 'Description.of.Topography', 'Topography.Code', 'Morphology.Description',\n",
    "                              'Code.of.Morphology', 'Description.of.Disease', 'Illness.Code', 'Diagnostic.means',\n",
    "                              'Extension',\n",
    "                              'Type.of.Death']\n",
    "\n",
    "for column in columns_to_one_hot:\n",
    "    top_9_values = X_train[column].value_counts().nlargest(9).index\n",
    "    X_train[column] = X_train[column].where(X_train[column].isin(top_9_values), 'other')\n",
    "    X_test[column] = X_test[column].where(X_test[column].isin(top_9_values), 'other')\n",
    "\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=columns_to_one_hot)\n",
    "X_test = pd.get_dummies(X_test, columns=columns_to_one_hot)\n",
    "\n",
    "print((X_test.columns == X_train.columns).all())\n",
    "\n",
    "columns_to_binarize = ['Gender', 'Indicator.of.Rare.Case']    \n",
    "for column in columns_to_binarize:\n",
    "    lb = LabelBinarizer()\n",
    "    X_train[column] = lb.fit_transform(X_train[column])\n",
    "    X_test[column] = lb.transform(X_test[column])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train['Age'] = scaler.fit_transform(X_train[['Age']])\n",
    "X_test['Age'] = scaler.transform(X_test[['Age']])\n",
    "\n",
    "Tmax = time_train.max()\n",
    "num_intervals=7\n",
    "intervals = [(i * (Tmax // num_intervals), (i + 1) * (Tmax // num_intervals)) for i in range(num_intervals)]\n",
    "\n",
    "Y_train = np.zeros((len(time_train), num_intervals), dtype=np.int_)\n",
    "# Until the event happens, value is 1. after that, it is 0\n",
    "for i, time_val in enumerate(time_train):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if time_val > right or (left < time_val <= right):\n",
    "            Y_train[i, j] = 1\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "\n",
    "\n",
    "Y_test = np.zeros((len(time_test), num_intervals), dtype=np.int_)\n",
    "# Until the event happens, value is 1. after that, it is 0\n",
    "for i, time_val in enumerate(time_test):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if time_val > right or (left < time_val <= right):\n",
    "            Y_test[i, j] = 1\n",
    "Y_test = torch.Tensor(Y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Creat mask matrix\n",
    "W_train = np.zeros((len(time_train), num_intervals), dtype=np.int_)\n",
    "# Until the time we know he was alive the value is 1, after that is 0\n",
    "for i, (time_val, event_val) in enumerate(zip(time_train, event_train)):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if event_val == 0 and time_val < left:\n",
    "            W_train[i, j] = 0\n",
    "        else:\n",
    "            W_train[i, j] = 1\n",
    "W_train = torch.Tensor(W_train)\n",
    "\n",
    "\n",
    "\n",
    "# Creat mask matrix\n",
    "W_test = np.zeros((len(time_test), num_intervals), dtype=np.int_)\n",
    "# Until the time we know he was alive the value is 1, after that is 0\n",
    "for i, (time_val, event_val) in enumerate(zip(time_test, event_test)):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if event_val == 0 and time_val < left:\n",
    "            W_test[i, j] = 0\n",
    "        else:\n",
    "            W_test[i, j] = 1\n",
    "W_test = torch.Tensor(W_test)\n",
    "\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == bool:\n",
    "        X_train[col] = X_train[col].astype(int)\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].dtype == bool:\n",
    "        X_test[col] = X_test[col].astype(int)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Y_train = Y[train_indices]\n",
    "# Y_test = Y[test_indices]\n",
    "# W_train = W[train_indices]\n",
    "# W_test  = W[test_indices]\n",
    "Y_train_transform = [Y_train[:, i:i + 1] for i in range(Y_train.size(1))]\n",
    "Y_test_transform = [Y_test[:, i:i + 1] for i in range(Y_test.size(1))]\n",
    "W_train_transform = [W_train[:, i:i+1] for i in range(W_train.size(1))]\n",
    "W_test_transform = [W_test[:, i:i+1] for i in range(W_test.size(1))]\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.01,\n",
    "    \"epochs\": 200,\n",
    "    \"clip\": 5.0,\n",
    "    \"lambda_reg\": 0.01,\n",
    "    \"save_path\": \"outputfiles\",\n",
    "    \"eg_k\" : 1, \n",
    "    \"early_stop_patience\":15,\n",
    "})\n",
    "\n",
    "train_dataset = MultiTaskDataset(X_train, Y_train_transform, W_train_transform, event_train)\n",
    "test_dataset = MultiTaskDataset(X_test, Y_test_transform, W_test_transform, event_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "model = MultiTaskModel(X_train.shape[1], Y_train.shape[1])\n",
    "\n",
    "\n",
    "trainer = NoRegularizationTrainer(model,train_loader,test_loader, args)\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:43:58.013963200Z",
     "start_time": "2024-06-23T21:43:23.405309Z"
    }
   },
   "id": "3d229653a60a0003"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T20:58:53.707094200Z",
     "start_time": "2024-06-23T20:58:53.681065300Z"
    }
   },
   "id": "d3815def69dfe1de"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint at epoch 9 with best validation loss 0.9998\n",
      "C-index for Training Data: 0.9791\n",
      "C-index for Test Data: 0.9779\n"
     ]
    }
   ],
   "source": [
    "class CindexOptimized(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(CindexOptimized, self).__init__()\n",
    "\n",
    "    def forward(self, y, y_hat, status):\n",
    "        if not torch.is_tensor(y):\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        if not torch.is_tensor(y_hat):\n",
    "            y_hat = torch.tensor(y_hat, dtype=torch.float32)\n",
    "        if not torch.is_tensor(status):\n",
    "            status = torch.tensor(status, dtype=torch.float32)\n",
    "\n",
    "        # replacing loop acceleration with matrix calculation\n",
    "\n",
    "        y_diff = y.unsqueeze(1) - y.unsqueeze(0)\n",
    "        y_hat_diff = y_hat.unsqueeze(1) - y_hat.unsqueeze(0)\n",
    "        # status[i] and status[j] mark whether to censored data\n",
    "        status_i = status.unsqueeze(1)\n",
    "        status_j = status.unsqueeze(0)\n",
    "        valid_pairs = torch.logical_or((y_diff <= 0) & (status_i == 1), (y_diff >= 0) & (status_j == 1)).float()\n",
    "        torch.diagonal(valid_pairs).fill_(0) #Diagonal set to 0 to eliminate interference\n",
    "        concordant_pairs = torch.logical_or((y_diff <= 0) & (y_hat_diff <= 0)&(status_i == 1),(y_diff >= 0) & (y_hat_diff >= 0)& (status_j == 1)).float()\n",
    "        torch.diagonal(concordant_pairs).fill_(0) #Diagonal set to 0 to eliminate interference\n",
    "        concordant_pairs = concordant_pairs.float()\n",
    "        c_index = concordant_pairs.sum() / valid_pairs.sum()\n",
    "        return c_index.item()\n",
    "\n",
    "\n",
    "\n",
    "cindex_calculator_optimized = CindexOptimized()\n",
    "\n",
    "trainer.load_best_checkpoint()\n",
    "predictions, Y_hat, Y_true, events = trainer.predict(train_loader)\n",
    "c11_train = cindex_calculator_optimized(Y_true, Y_hat, events)\n",
    "print(f\"C-index for Training Data: {c11_train:.4f}\")\n",
    "\n",
    "\n",
    "predictions, Y_hat, Y_true, events = trainer.predict(test_loader)\n",
    "c11_test = cindex_calculator_optimized(Y_true, Y_hat, events)\n",
    "print(f\"C-index for Test Data: {c11_test:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:45:55.594505100Z",
     "start_time": "2024-06-23T21:45:53.817104600Z"
    }
   },
   "id": "372ea3fc3a479a40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "843b7e67e315f459"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "Y = np.zeros((len(time_all), num_intervals), dtype=np.int_)\n",
    "# Until the event happens, value is 1. after that, it is 0\n",
    "for i, time_val in enumerate(time_all):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if time_val > right or (left < time_val <= right):\n",
    "            Y[i, j] = 1\n",
    "Y = torch.Tensor(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:46:15.301947700Z",
     "start_time": "2024-06-23T21:46:15.257951600Z"
    }
   },
   "id": "47e57da9e278766e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint at epoch 9 with best validation loss 0.9998\n",
      "Evaluation On Train Data \n",
      "Y_true Train\n",
      "Unique Values: tensor([0., 1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64)\n",
      "Counts: tensor([3064, 3004, 1193, 1548, 1232,  714,  200,   53])\n",
      "Y_hat Train\n",
      "Unique Values: tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32)\n",
      "Counts: tensor([2672, 2415, 1061, 1071,  488,  267,    1, 3033])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11008/11008 [00:01<00:00, 7446.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data C-Index = 0.9790862202644348,  BScore = 0.06266008509244994\n",
      "\n",
      "Evaluation On Test Data \n",
      "\n",
      "Y_true Test\n",
      "Unique Values: tensor([0., 1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64)\n",
      "Counts: tensor([1026, 1004,  395,  496,  410,  223,   74,   20])\n",
      "Y_hat Test\n",
      "Unique Values: tensor([0, 1, 2, 3, 4, 5, 7], dtype=torch.int32)\n",
      "Counts: tensor([ 890,  766,  381,  353,  174,   84, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3648/3648 [00:00<00:00, 7982.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data C-Index = 0.9779160618782043,  BScore = 0.06779034798891784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.load_best_checkpoint()\n",
    "\n",
    "print(\"Evaluation On Train Data \\n\")\n",
    "\n",
    "predictions, Y_hat, Y_true, events = trainer.predict(train_loader)\n",
    "print(\"Y_true Train\")\n",
    "unique_value_counts(Y_true)\n",
    "print(\"Y_hat Train\")\n",
    "unique_value_counts(Y_hat)\n",
    "\n",
    "cindex_calculator = Cindex()\n",
    "cindex_test = cindex_calculator(Y_true, Y_hat, events)\n",
    "bscore_test = brier_score(event_all, Y, events, Y_true, predictions)\n",
    "print(f\"Train Data C-Index = {cindex_test},  BScore = {bscore_test}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nEvaluation On Test Data \\n\")\n",
    "\n",
    "predictions, Y_hat, Y_true, events = trainer.predict(test_loader)\n",
    "print(\"Y_true Test\")\n",
    "unique_value_counts(Y_true)\n",
    "print(\"Y_hat Test\")\n",
    "unique_value_counts(Y_hat)\n",
    "\n",
    "cindex_calculator = Cindex()\n",
    "cindex_test = cindex_calculator(Y_true, Y_hat, events)\n",
    "bscore_test = brier_score(event_all, Y, events, Y_true, predictions)\n",
    "print(f\"Test Data C-Index = {cindex_test},  BScore = {bscore_test}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:46:18.876230500Z",
     "start_time": "2024-06-23T21:46:15.916974800Z"
    }
   },
   "id": "b4bf80a46cfe5bb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f246ed966b77f232"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest C-Index : 0.9458985643111396\n",
      "Random Forest Integrated Brier Score : 0.14854045547856617\n",
      "Random Forest C-Index : 0.9449668740302326\n",
      "Random Forest Integrated Brier Score : 0.1450701356881438\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "\n",
    "Y1 = Y.sum(axis=1).cpu().numpy()\n",
    "dtype = [('cens', bool), ('time', int)]\n",
    "Ye = np.array(list(zip(event_all.astype(bool), Y1)), dtype=dtype)\n",
    "y_train, y_test = Y_train.sum(axis=1).cpu().numpy(), Y_test.sum(axis=1).cpu().numpy()\n",
    "\n",
    "y_train = np.array(list(zip(event_train.astype(bool), y_train)), dtype=dtype)\n",
    "y_test =np.array(list(zip(event_test.astype(bool), y_test)), dtype=dtype)\n",
    "\n",
    "rsf = RandomSurvivalForest(max_depth=2, random_state=1)\n",
    "rsf.fit(X_train, y_train)\n",
    "print(f\"Random Forest C-Index : {rsf.score(X_test, y_test)}\")\n",
    "survs = rsf.predict_survival_function(X_test)\n",
    "_times = np.arange(1, 7)\n",
    "preds = np.asarray([[fn(t) for t in _times] for fn in survs])\n",
    "print(f\"Random Forest Integrated Brier Score : {integrated_brier_score(Ye, y_test, preds, _times)}\")\n",
    "print(f\"Random Forest C-Index : {rsf.score(X_train, y_train)}\")\n",
    "survs = rsf.predict_survival_function(X_train)\n",
    "_times = np.arange(1, 7)\n",
    "preds = np.asarray([[fn(t) for t in _times] for fn in survs])\n",
    "print(f\"Random Forest Integrated Brier Score : {integrated_brier_score(Ye, y_train, preds, _times)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:47:14.369684900Z",
     "start_time": "2024-06-23T21:47:12.275158100Z"
    }
   },
   "id": "58bacf83c899932f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "98804ea3d73a180b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
