{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-01T15:14:39.920645400Z",
     "start_time": "2024-07-01T15:14:38.059645500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from algorithm import (\n",
    "                        NoRegularizationTrainer,\n",
    "                        GradientTrainer,\n",
    "                        EGTrainer,\n",
    "                        CSVDataLoader,\n",
    "                        MultiTaskModel,\n",
    "                        MultiTaskDataset,\n",
    "                        mlt_train_test_split,\n",
    "                        # true_values_from_data_loader,\n",
    "                        unique_value_counts,\n",
    "                        Cindex,\n",
    "                        brier_score,\n",
    "                        )\n",
    "import easydict\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e4b00ac7b2dbe00c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10f19944d16e58a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset/processdata.csv', encoding='latin-1')\n",
    "date_columns = ['Date.of.Last.Contact', 'Date.of.Diagnostic']\n",
    "data[date_columns] = data[date_columns].apply(pd.to_datetime, errors='coerce')\n",
    "data['Survival_Time'] = (data['Date.of.Last.Contact'] - data['Date.of.Diagnostic']).dt.days\n",
    "data.loc[:, 'Survival_Time'] = data['Survival_Time'].replace({-1: 0})\n",
    "data['indicater'] = np.where(data['Date.of.Death'].isna(), 0, 1)\n",
    "\n",
    "\n",
    "columns_to_drop = ['Date.of.Death', 'Date.of.Last.Contact', 'Date.of.Diagnostic',\n",
    "                   'Type.of.Death',  # High Correlation with 'Date.of.Death' and events\n",
    "                   ]\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "# print(data.isna().sum())\n",
    "# data.dropna(inplace=True)\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "        range(len(data)), stratify=data['indicater'], random_state=1, test_size=0.25\n",
    "    )\n",
    "\n",
    "X = data.drop(['Survival_Time', 'indicater'], axis=1)\n",
    "time_all = data['Survival_Time'].values\n",
    "event_all = data['indicater'].values\n",
    "\n",
    "X_train = X.iloc[train_indices].copy()\n",
    "X_test = X.iloc[test_indices].copy()\n",
    "time_train = time_all[train_indices].copy()\n",
    "time_test = time_all[test_indices].copy()\n",
    "event_train = event_all[train_indices].copy()\n",
    "event_test = event_all[test_indices].copy()\n",
    "\n",
    "\n",
    "columns_to_one_hot = ['RCBP.Name', 'Raca.Color', 'State.Civil', 'Code.Profession', 'Name.Occupation',\n",
    "                              'Status.Address',\n",
    "                              'City.Address', 'Description.of.Topography', 'Topography.Code', 'Morphology.Description',\n",
    "                              'Code.of.Morphology', 'Description.of.Disease', 'Illness.Code', 'Diagnostic.means',\n",
    "                              'Extension',\n",
    "                              #'Type.of.Death' # High Correlation with 'Date.of.Death' and events\n",
    "                      ]\n",
    "\n",
    "for column in columns_to_one_hot:\n",
    "    top_9_values = X_train[column].value_counts().nlargest(9).index\n",
    "    X_train[column] = X_train[column].where(X_train[column].isin(top_9_values), 'other')\n",
    "    X_test[column] = X_test[column].where(X_test[column].isin(top_9_values), 'other')\n",
    "\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=columns_to_one_hot)\n",
    "X_test = pd.get_dummies(X_test, columns=columns_to_one_hot)\n",
    "\n",
    "print((X_test.columns == X_train.columns).all())\n",
    "\n",
    "columns_to_binarize = ['Gender', 'Indicator.of.Rare.Case']    \n",
    "for column in columns_to_binarize:\n",
    "    lb = LabelBinarizer()\n",
    "    X_train[column] = lb.fit_transform(X_train[column])\n",
    "    X_test[column] = lb.transform(X_test[column])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train['Age'] = scaler.fit_transform(X_train[['Age']])\n",
    "X_test['Age'] = scaler.transform(X_test[['Age']])\n",
    "\n",
    "Tmax = time_train.max()\n",
    "num_intervals=7\n",
    "intervals = [(i * (Tmax // num_intervals), (i + 1) * (Tmax // num_intervals)) for i in range(num_intervals)]\n",
    "\n",
    "Y_train = np.zeros((len(time_train), num_intervals), dtype=np.int_)\n",
    "# Until the event happens, value is 1. after that, it is 0\n",
    "for i, time_val in enumerate(time_train):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if time_val > right or (left < time_val <= right):\n",
    "            Y_train[i, j] = 1\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "\n",
    "\n",
    "Y_test = np.zeros((len(time_test), num_intervals), dtype=np.int_)\n",
    "# Until the event happens, value is 1. after that, it is 0\n",
    "for i, time_val in enumerate(time_test):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if time_val > right or (left < time_val <= right):\n",
    "            Y_test[i, j] = 1\n",
    "Y_test = torch.Tensor(Y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Creat mask matrix\n",
    "W_train = np.zeros((len(time_train), num_intervals), dtype=np.int_)\n",
    "# Until the time we know he was alive the value is 1, after that is 0\n",
    "for i, (time_val, event_val) in enumerate(zip(time_train, event_train)):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if event_val == 0 and time_val < left:\n",
    "            W_train[i, j] = 0\n",
    "        else:\n",
    "            W_train[i, j] = 1\n",
    "W_train = torch.Tensor(W_train)\n",
    "\n",
    "\n",
    "\n",
    "# Creat mask matrix\n",
    "W_test = np.zeros((len(time_test), num_intervals), dtype=np.int_)\n",
    "# Until the time we know he was alive the value is 1, after that is 0\n",
    "for i, (time_val, event_val) in enumerate(zip(time_test, event_test)):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if event_val == 0 and time_val < left:\n",
    "            W_test[i, j] = 0\n",
    "        else:\n",
    "            W_test[i, j] = 1\n",
    "W_test = torch.Tensor(W_test)\n",
    "\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == bool:\n",
    "        X_train[col] = X_train[col].astype(int)\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].dtype == bool:\n",
    "        X_test[col] = X_test[col].astype(int)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Y_train = Y[train_indices]\n",
    "# Y_test = Y[test_indices]\n",
    "# W_train = W[train_indices]\n",
    "# W_test  = W[test_indices]\n",
    "Y_train_transform = [Y_train[:, i:i + 1] for i in range(Y_train.size(1))]\n",
    "Y_test_transform = [Y_test[:, i:i + 1] for i in range(Y_test.size(1))]\n",
    "W_train_transform = [W_train[:, i:i+1] for i in range(W_train.size(1))]\n",
    "W_test_transform = [W_test[:, i:i+1] for i in range(W_test.size(1))]\n",
    "\n",
    "print((W_test_transform[-1] == 0).any())\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.01,\n",
    "    \"epochs\": 200,\n",
    "    \"clip\": 5.0,\n",
    "    \"lambda_reg\": 0.01,\n",
    "    \"save_path\": \"outputfiles\",\n",
    "    \"eg_k\" : 1, \n",
    "    \"early_stop_patience\":15,\n",
    "})\n",
    "\n",
    "train_dataset = MultiTaskDataset(X_train, Y_train_transform, W_train_transform, event_train)\n",
    "test_dataset = MultiTaskDataset(X_test, Y_test_transform, W_test_transform, event_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502e4c2ec860b444"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EditedCindexOptimized(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(EditedCindexOptimized, self).__init__()\n",
    "\n",
    "    def forward(self, y, y_hat, status):\n",
    "        if not torch.is_tensor(y):\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        if not torch.is_tensor(y_hat):\n",
    "            y_hat = torch.tensor(y_hat, dtype=torch.float32)\n",
    "        if not torch.is_tensor(status):\n",
    "            status = torch.tensor(status, dtype=torch.float32)\n",
    "\n",
    "        # replacing loop acceleration with matrix calculation\n",
    "        \n",
    "        y_diff = y.unsqueeze(1) - y.unsqueeze(0)\n",
    "        y_hat_diff = y_hat.unsqueeze(1) - y_hat.unsqueeze(0)\n",
    "        # status[i] and status[j] mark whether to censored data\n",
    "        status_i = status.unsqueeze(1)\n",
    "        status_j = status.unsqueeze(0)\n",
    "        valid_pairs = torch.logical_or((y_diff < 0) & (status_i == 1), (y_diff > 0) & (status_j == 1)).float()\n",
    "        torch.diagonal(valid_pairs).fill_(0) #Diagonal set to 0 to eliminate interference\n",
    "        concordant_pairs = torch.logical_or((y_diff < 0) & (y_hat_diff < 0)&(status_i == 1),(y_diff > 0) & (y_hat_diff > 0)& (status_j == 1)).float()\n",
    "        torch.diagonal(concordant_pairs).fill_(0) #Diagonal set to 0 to eliminate interference\n",
    "        concordant_pairs = concordant_pairs.float()\n",
    "        c_index = concordant_pairs.sum() / valid_pairs.sum()\n",
    "        return c_index.item()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eba7a83357438fb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "107e9376e504ea17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results for Model with No regularization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cd001ba1e1230e2"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:01<04:03,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 0, Average Training Loss: 2.0818, Average Gradient Norm: 0.0000\n",
      "End of Epoch 0, Average Validation Loss: 1.9272\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:02<03:26,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 1, Average Training Loss: 1.8792, Average Gradient Norm: 0.0000\n",
      "End of Epoch 1, Average Validation Loss: 1.7902\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:03<03:17,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 2, Average Training Loss: 1.8353, Average Gradient Norm: 0.0000\n",
      "End of Epoch 2, Average Validation Loss: 1.8195\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:04<03:22,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 3, Average Training Loss: 1.7908, Average Gradient Norm: 0.0000\n",
      "End of Epoch 3, Average Validation Loss: 2.2562\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:05<03:17,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 4, Average Training Loss: 1.7691, Average Gradient Norm: 0.0000\n",
      "End of Epoch 4, Average Validation Loss: 1.8103\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [00:06<03:33,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 5, Average Training Loss: 1.7902, Average Gradient Norm: 0.0000\n",
      "End of Epoch 5, Average Validation Loss: 1.8150\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [00:07<03:24,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 6, Average Training Loss: 1.7499, Average Gradient Norm: 0.0000\n",
      "End of Epoch 6, Average Validation Loss: 1.9101\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:08<03:25,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 7, Average Training Loss: 1.7490, Average Gradient Norm: 0.0000\n",
      "End of Epoch 7, Average Validation Loss: 1.8215\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:09<03:43,  1.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m MultiTaskModel(X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], Y_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m NoRegularizationTrainer(model,train_loader,test_loader, args)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Data\\Development\\PHD\\Wang\\NLP\\algorithm\\no_reg_trainer.py:41\u001B[0m, in \u001B[0;36mNoRegularizationTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     38\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m     39\u001B[0m gradient_norms \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (X_train, targets, masks, event_train) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_loader):\n\u001B[0;32m     42\u001B[0m     X_train \u001B[38;5;241m=\u001B[39m X_train\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     43\u001B[0m     X_train\u001B[38;5;241m.\u001B[39mrequires_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\future\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\future\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\future\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\future\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\future\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\future\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\future\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    144\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\future\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    211\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    212\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = MultiTaskModel(X_train.shape[1], Y_train.shape[1])\n",
    "trainer = NoRegularizationTrainer(model,train_loader,test_loader, args)\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T15:34:35.140223100Z",
     "start_time": "2024-07-01T15:34:25.673019Z"
    }
   },
   "id": "26a8fe496efc7dce"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint at epoch 13 with best validation loss 1.6987\n",
      "C-index for Training Data: 0.8190\n",
      "C-index for Test Data: 0.8048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cindex_calculator_optimized = EditedCindexOptimized()\n",
    "\n",
    "trainer.load_best_checkpoint()\n",
    "predictions, Y_hat, Y_true, events = trainer.predict(train_loader)\n",
    "# Y_hat = predictions[np.arange(predictions.shape[0]), (Y_hat-1)]\n",
    "c11_train = cindex_calculator_optimized(Y_true, Y_hat, events)\n",
    "print(f\"C-index for Training Data: {c11_train:.4f}\")\n",
    "\n",
    "\n",
    "predictions, Y_hat, Y_true, events = trainer.predict(test_loader)\n",
    "# Y_hat = predictions[np.arange(predictions.shape[0]), (Y_hat-1)]\n",
    "c11_test = cindex_calculator_optimized(Y_true, Y_hat, events)\n",
    "print(f\"C-index for Test Data: {c11_test:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T15:27:52.173021400Z",
     "start_time": "2024-07-01T15:27:50.505022Z"
    }
   },
   "id": "725b53e732397c64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "74afbb430987f04d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results for EG Based Regularization Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e7fffcb5b82181b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:03<10:49,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 0, Average Training Loss: 2.5207, Average Gradient Norm: 2.3792\n",
      "End of Epoch 0, Average Validation Loss: 2.2120\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:06<10:37,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 1, Average Training Loss: 2.3999, Average Gradient Norm: 2.6376\n",
      "End of Epoch 1, Average Validation Loss: 2.0816\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:09<10:39,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 2, Average Training Loss: 2.3546, Average Gradient Norm: 2.2355\n",
      "End of Epoch 2, Average Validation Loss: 2.1455\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:12<10:30,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 3, Average Training Loss: 2.3448, Average Gradient Norm: 2.3484\n",
      "End of Epoch 3, Average Validation Loss: 1.9132\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:15<10:18,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 4, Average Training Loss: 2.3346, Average Gradient Norm: 2.4201\n",
      "End of Epoch 4, Average Validation Loss: 1.9334\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [00:19<10:14,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 5, Average Training Loss: 2.3498, Average Gradient Norm: 2.3907\n",
      "End of Epoch 5, Average Validation Loss: 1.9082\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [00:22<10:16,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 6, Average Training Loss: 2.3235, Average Gradient Norm: 2.4392\n",
      "End of Epoch 6, Average Validation Loss: 1.9261\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 7, Average Training Loss: 2.2987, Average Gradient Norm: 2.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:25<10:28,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 7, Average Validation Loss: 1.8973\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:29<10:20,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 8, Average Training Loss: 2.3112, Average Gradient Norm: 2.2496\n",
      "End of Epoch 8, Average Validation Loss: 1.9098\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [00:32<10:19,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 9, Average Training Loss: 2.3164, Average Gradient Norm: 2.2573\n",
      "End of Epoch 9, Average Validation Loss: 1.9390\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [00:35<10:27,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 10, Average Training Loss: 2.2907, Average Gradient Norm: 2.2124\n",
      "End of Epoch 10, Average Validation Loss: 1.9054\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [00:38<10:15,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 11, Average Training Loss: 2.3024, Average Gradient Norm: 2.3354\n",
      "End of Epoch 11, Average Validation Loss: 1.9301\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 13/200 [00:42<10:06,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 12, Average Training Loss: 2.2879, Average Gradient Norm: 2.2790\n",
      "End of Epoch 12, Average Validation Loss: 1.8231\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/200 [00:45<10:19,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 13, Average Training Loss: 2.3132, Average Gradient Norm: 2.3963\n",
      "End of Epoch 13, Average Validation Loss: 1.8418\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15/200 [00:48<10:16,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 14, Average Training Loss: 2.3199, Average Gradient Norm: 2.5561\n",
      "End of Epoch 14, Average Validation Loss: 1.8393\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [00:52<10:28,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 15, Average Training Loss: 2.3451, Average Gradient Norm: 2.9854\n",
      "End of Epoch 15, Average Validation Loss: 2.2049\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 17/200 [00:55<10:22,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 16, Average Training Loss: 2.2943, Average Gradient Norm: 2.6413\n",
      "End of Epoch 16, Average Validation Loss: 2.0316\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [00:59<10:26,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 17, Average Training Loss: 2.2801, Average Gradient Norm: 2.2977\n",
      "End of Epoch 17, Average Validation Loss: 1.8571\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 18, Average Training Loss: 2.2855, Average Gradient Norm: 2.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [01:02<10:24,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 18, Average Validation Loss: 2.0419\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [01:06<10:13,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 19, Average Training Loss: 2.2142, Average Gradient Norm: 2.1280\n",
      "End of Epoch 19, Average Validation Loss: 1.8390\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [01:09<10:11,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 20, Average Training Loss: 2.1902, Average Gradient Norm: 2.3427\n",
      "End of Epoch 20, Average Validation Loss: 1.8704\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 22/200 [01:13<10:05,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 21, Average Training Loss: 2.1840, Average Gradient Norm: 2.4634\n",
      "End of Epoch 21, Average Validation Loss: 1.8956\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [01:16<09:50,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 22, Average Training Loss: 2.1781, Average Gradient Norm: 2.6029\n",
      "End of Epoch 22, Average Validation Loss: 1.8065\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 24/200 [01:19<09:44,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 23, Average Training Loss: 2.1657, Average Gradient Norm: 2.6927\n",
      "End of Epoch 23, Average Validation Loss: 1.8628\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 25/200 [01:22<09:38,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 24, Average Training Loss: 2.1636, Average Gradient Norm: 2.7767\n",
      "End of Epoch 24, Average Validation Loss: 1.8169\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 26/200 [01:26<09:39,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 25, Average Training Loss: 2.1667, Average Gradient Norm: 2.8504\n",
      "End of Epoch 25, Average Validation Loss: 1.8081\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 27/200 [01:29<09:33,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 26, Average Training Loss: 2.1656, Average Gradient Norm: 2.9833\n",
      "End of Epoch 26, Average Validation Loss: 1.7836\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [01:32<09:22,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 27, Average Training Loss: 2.1623, Average Gradient Norm: 2.9860\n",
      "End of Epoch 27, Average Validation Loss: 1.7916\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 29/200 [01:36<09:27,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 28, Average Training Loss: 2.1656, Average Gradient Norm: 3.2259\n",
      "End of Epoch 28, Average Validation Loss: 1.8270\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30/200 [01:39<09:25,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 29, Average Training Loss: 2.1636, Average Gradient Norm: 3.0586\n",
      "End of Epoch 29, Average Validation Loss: 1.7756\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 31/200 [01:42<09:07,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 30, Average Training Loss: 2.1570, Average Gradient Norm: 3.0924\n",
      "End of Epoch 30, Average Validation Loss: 1.7913\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 32/200 [01:45<09:09,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 31, Average Training Loss: 2.1532, Average Gradient Norm: 3.1800\n",
      "End of Epoch 31, Average Validation Loss: 1.7697\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 32, Average Training Loss: 2.1685, Average Gradient Norm: 3.4244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 33/200 [01:49<09:08,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 32, Average Validation Loss: 1.8188\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 33, Average Training Loss: 2.1599, Average Gradient Norm: 3.1676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 34/200 [01:52<09:16,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 33, Average Validation Loss: 1.8449\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 35/200 [01:55<09:02,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 34, Average Training Loss: 2.1487, Average Gradient Norm: 3.4859\n",
      "End of Epoch 34, Average Validation Loss: 1.8955\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 36/200 [01:59<08:58,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 35, Average Training Loss: 2.1610, Average Gradient Norm: 3.4343\n",
      "End of Epoch 35, Average Validation Loss: 1.8860\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 37/200 [02:02<08:59,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 36, Average Training Loss: 2.1597, Average Gradient Norm: 3.2599\n",
      "End of Epoch 36, Average Validation Loss: 1.8063\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 37, Average Training Loss: 2.1626, Average Gradient Norm: 3.3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 38/200 [02:08<11:16,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 37, Average Validation Loss: 1.8163\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 38, Average Training Loss: 2.1429, Average Gradient Norm: 3.2199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 39/200 [02:15<13:10,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 38, Average Validation Loss: 1.8024\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 39, Average Training Loss: 2.1478, Average Gradient Norm: 3.2975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [02:21<14:28,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 39, Average Validation Loss: 1.8077\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 40, Average Training Loss: 2.1404, Average Gradient Norm: 3.2129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [02:29<15:46,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 40, Average Validation Loss: 1.8039\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 41, Average Training Loss: 2.1636, Average Gradient Norm: 3.3223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 42/200 [02:35<16:09,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 41, Average Validation Loss: 1.8044\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 42, Average Training Loss: 2.1511, Average Gradient Norm: 3.2687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 43/200 [02:42<16:44,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 42, Average Validation Loss: 1.8050\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 43, Average Training Loss: 2.1410, Average Gradient Norm: 3.3286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 44/200 [02:49<17:13,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 43, Average Validation Loss: 1.8193\n",
      "Current Learning Rate: 0.000010\n",
      "End of Epoch 44, Average Training Loss: 2.1466, Average Gradient Norm: 3.3104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 45/200 [02:57<17:39,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 44, Average Validation Loss: 1.8084\n",
      "Current Learning Rate: 0.000010\n",
      "End of Epoch 45, Average Training Loss: 2.1430, Average Gradient Norm: 3.3672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 46/200 [03:04<17:42,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 45, Average Validation Loss: 1.8060\n",
      "Current Learning Rate: 0.000010\n",
      "End of Epoch 46, Average Training Loss: 2.1492, Average Gradient Norm: 3.2385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 46/200 [03:11<10:40,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 46, Average Validation Loss: 1.8025\n",
      "Current Learning Rate: 0.000010\n",
      "Early stopping triggered after 47 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultiTaskModel(X_train.shape[1], Y_train.shape[1])\n",
    "trainer = EGTrainer(model,train_loader,test_loader, train_dataset, args)\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T15:37:51.388864Z",
     "start_time": "2024-07-01T15:34:40.136707Z"
    }
   },
   "id": "9f4ae92ff570300b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint at epoch 32 with best validation loss 1.7697\n",
      "C-index for Training Data: 0.8131\n",
      "C-index for Test Data: 0.8045\n"
     ]
    }
   ],
   "source": [
    "trainer.load_best_checkpoint()\n",
    "predictions, Y_hat, Y_true, events = trainer.predict(train_loader)\n",
    "# Y_hat = predictions[np.arange(predictions.shape[0]), (Y_hat-1)]\n",
    "c11_train = cindex_calculator_optimized(Y_true, Y_hat, events)\n",
    "print(f\"C-index for Training Data: {c11_train:.4f}\")\n",
    "\n",
    "\n",
    "predictions, Y_hat, Y_true, events = trainer.predict(test_loader)\n",
    "# Y_hat = predictions[np.arange(predictions.shape[0]), (Y_hat-1)]\n",
    "c11_test = cindex_calculator_optimized(Y_true, Y_hat, events)\n",
    "print(f\"C-index for Test Data: {c11_test:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-01T15:38:33.627656900Z",
     "start_time": "2024-07-01T15:38:32.024657600Z"
    }
   },
   "id": "18866a281ffe5bb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7a98fc25f2544fdb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
