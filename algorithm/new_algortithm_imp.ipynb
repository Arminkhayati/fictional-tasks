{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:45:22.680137100Z",
     "start_time": "2024-06-20T23:45:20.833135800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import numpy as np\n",
    "import easydict\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import numpy as np\n",
    "import easydict\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from expected_gradient_multi_task import AttributionPriorExplainer\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.autograd import grad\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7408\n",
      "torch.Size([14755, 7])\n",
      "torch.Size([14755, 1])\n",
      "torch.Size([14755, 1])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/processdata.csv', encoding='latin-1')\n",
    "data = df\n",
    "\n",
    "date_columns = ['Date.of.Last.Contact', 'Date.of.Diagnostic']\n",
    "data[date_columns] = data[date_columns].apply(pd.to_datetime, errors='coerce')\n",
    "has_na = data[date_columns].isna().any(axis=1)\n",
    "\n",
    "# My comment: why not death time - contact time?\n",
    "if has_na.any():\n",
    "    data['Survival_Time'] = (data['Date.of.Last.Contact'] - data['Date.of.Diagnostic']).dt.days\n",
    "else:\n",
    "    data['Survival_Time'] = (data['Date.of.Last.Contact'] - data['Date.of.Diagnostic']).dt.days\n",
    "    \n",
    "data.loc[:, 'Survival_Time'] = data['Survival_Time'].replace({-1:0})\n",
    "\n",
    "data['indicater'] = np.where(data['Date.of.Death'].isna(), 0, 1)\n",
    "columns_to_drop = ['Date.of.Death', 'Date.of.Last.Contact', 'Date.of.Diagnostic']\n",
    "\n",
    "\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "columns_to_one_hot = ['RCBP.Name', 'Raca.Color', 'State.Civil', 'Code.Profession', 'Name.Occupation', 'Status.Address',\n",
    "                      'City.Address', 'Description.of.Topography', 'Topography.Code', 'Morphology.Description',\n",
    "                      'Code.of.Morphology', 'Description.of.Disease', 'Illness.Code', 'Diagnostic.means', 'Extension',\n",
    "                      'Type.of.Death']\n",
    "\n",
    "# Replace other values that are not in top 9, into \"other\"\n",
    "for column in columns_to_one_hot:\n",
    "    top_9_values = data[column].value_counts().nlargest(9).index\n",
    "    data[column] = data[column].where(data[column].isin(top_9_values), 'other')\n",
    "\n",
    "data = pd.get_dummies(data, columns=columns_to_one_hot)\n",
    "\n",
    "columns_to_binarize = ['Gender', 'Indicator.of.Rare.Case']\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "for column in columns_to_binarize:\n",
    "    data[column] = lb.fit_transform(data[column])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data['Age'] = scaler.fit_transform(data[['Age']])\n",
    "\n",
    "\n",
    "\n",
    "X = data.drop(['Survival_Time', 'indicater'], axis=1)\n",
    "time_all = data['Survival_Time'].values\n",
    "event_all = data['indicater'].values\n",
    "max_time = data['Survival_Time'].max()\n",
    "print(max_time)\n",
    "\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Take the maximum time and divide it into any number of intervals\n",
    "Tmax = 7500\n",
    "num_intervals = 7\n",
    "\n",
    "intervals = [(i * (Tmax // num_intervals), (i + 1) * (Tmax // num_intervals)) for i in range(num_intervals)]\n",
    "\n",
    "Y = np.zeros((len(time_all), num_intervals), dtype=np.int_)\n",
    "\n",
    "# Until the event happens, value is 1. after that, it is 0\n",
    "for i, time_val in enumerate(time_all):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if time_val > right or (left < time_val <= right):\n",
    "            Y[i, j] = 1\n",
    "\n",
    "Y = torch.Tensor(Y)\n",
    "\n",
    "print(Y.shape)\n",
    "\n",
    "# Creat mask matrix\n",
    "W = np.zeros((len(time_all), num_intervals), dtype=np.int_)\n",
    "\n",
    "# Until the time we know he was alive the value is 1, after that is 0\n",
    "for i, (time_val, event_val) in enumerate(zip(time_all, event_all)):\n",
    "    for j, (left, right) in enumerate(intervals):\n",
    "        if event_val == 0 and time_val < left:\n",
    "            W[i, j] = 0\n",
    "        else:\n",
    "            W[i, j] = 1\n",
    "\n",
    "W = torch.Tensor(W)\n",
    "\n",
    "\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == bool:\n",
    "        X[col] = X[col].astype(int)\n",
    "X_use = torch.tensor(X.values, dtype=torch.float32)\n",
    "X_use.shape\n",
    "\n",
    "# Convert n*m data into m * n * 1 \n",
    "Y_transform = [Y[:, i:i+1] for i in range(Y.size(1))]\n",
    "print(Y_transform[0].shape)\n",
    "W_transform = [W[:, i:i+1] for i in range(W.size(1))]\n",
    "print(W_transform[0].shape)\n",
    "\n",
    "\n",
    "# Jingyan adds the dataloader\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, data, targets, masks, event_all):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.masks = masks\n",
    "        self.event_all = event_all\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], [self.targets[i][idx] for i in range(len(self.targets))], [self.masks[i][idx] for i in range(len(self.masks))], self.event_all[idx]\n",
    "\n",
    "\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:45:22.880134300Z",
     "start_time": "2024-06-20T23:45:22.687135400Z"
    }
   },
   "id": "8625b02a773258fa"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 138])\n",
      "7 torch.Size([64, 1])\n",
      "7 torch.Size([64, 1])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#Xinyu\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"batch_size\": 64,\n",
    "    \"cuda\": True, # should set it to be true when using gpu, otherwise data would be on two devices\n",
    "    \"lr\": 0.05,\n",
    "    \"seed\": 1111,\n",
    "    \"reduce_rate\": 0.95,\n",
    "    \"epochs\": 200,\n",
    "    \"clip\": 5.0,\n",
    "    \"log_interval\":10,\n",
    "})\n",
    "\n",
    "full_dataset = MultiTaskDataset(X_use, Y_transform, W_transform, event_all)\n",
    "\n",
    "# train_size = int(0.8 * len(full_dataset))\n",
    "# test_size = len(full_dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.arange(len(Y)), stratify=event_all, random_state=1\n",
    ")\n",
    "train_dataset, test_dataset = (torch.utils.data.Subset(full_dataset, train_indices), \n",
    "                               torch.utils.data.Subset(full_dataset, test_indices))\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "for x, y, w, e in train_loader:\n",
    "    print(x.shape)\n",
    "    print(len(y), y[0].shape)\n",
    "    print(len(w), w[0].shape)\n",
    "    print(e.shape)\n",
    "    in_features = x.shape[1]\n",
    "    out_features = len(y)\n",
    "    break\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:45:22.905134800Z",
     "start_time": "2024-06-20T23:45:22.879134600Z"
    }
   },
   "id": "eef2592946c70ed6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Made by Xinyu and modified by Dr.Li and Jingyan\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.num_tasks = out_features\n",
    "        self.input_features = in_features\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.AlphaDropout(0.1),\n",
    "\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.AlphaDropout(0.1),\n",
    "\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.AlphaDropout(0.1)\n",
    "        )\n",
    "        # A Classic Multi Task Learning Framework\n",
    "        self.task_layers = nn.ModuleList([nn.Linear(128, 1) for _ in range(out_features)])#after your papaer I decided to use MTL to solve this prob\n",
    "\n",
    "    # This forward propagation logic defines the chain propagation of our idea\n",
    "    def forward(self, x):\n",
    "        shared_output = self.shared_layers(x)\n",
    "        task_outputs = []\n",
    "        for i, task_layer in enumerate(self.task_layers):\n",
    "            if i == 0:\n",
    "                task_output = torch.sigmoid(task_layer(shared_output))\n",
    "            else:\n",
    "                task_output = torch.sigmoid(task_layer(shared_output)) * task_outputs[-1]\n",
    "            task_outputs.append(task_output)\n",
    "\n",
    "        return task_outputs# Here I difined the S(x)\n",
    "\n",
    "    # Modified by Jingyan\n",
    "    def custom_loss(self, task_outputs, targets, masks):\n",
    "        loss = 0\n",
    "        for i, task_output in enumerate(task_outputs):\n",
    "            task_target = targets[i]\n",
    "            task_mask = masks[i]\n",
    "            task_loss = F.binary_cross_entropy(task_output, task_target.float(), reduction='none')\n",
    "            task_loss = task_loss * task_mask.float() # [2048, 1]\n",
    "            # print('task_loss', task_loss.shape)\n",
    "            loss += task_loss.sum() / task_mask.sum()\n",
    "        return loss\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:45:22.953140700Z",
     "start_time": "2024-06-20T23:45:22.899134200Z"
    }
   },
   "id": "efd8413f78c50e15"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:04<15:19,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 0, Average Training Loss: 2.0786, Average Gradient Norm: 2.6651\n",
      "End of Epoch 0, Average Validation Loss: 1.2684\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:07<12:23,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 1, Average Training Loss: 1.7817, Average Gradient Norm: 3.5047\n",
      "End of Epoch 1, Average Validation Loss: 1.3096\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 2, Average Training Loss: 1.6873, Average Gradient Norm: 3.3423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:10<11:27,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 2, Average Validation Loss: 1.1645\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 3, Average Training Loss: 1.6657, Average Gradient Norm: 3.5790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:17<14:48,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 3, Average Validation Loss: 1.2596\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 4, Average Training Loss: 1.6695, Average Gradient Norm: 3.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:24<17:46,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 4, Average Validation Loss: 1.3298\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 5, Average Training Loss: 1.6847, Average Gradient Norm: 3.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [00:31<19:45,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 5, Average Validation Loss: 1.2377\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 6, Average Training Loss: 1.6583, Average Gradient Norm: 4.2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [00:38<20:40,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 6, Average Validation Loss: 1.2907\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 7, Average Training Loss: 1.6943, Average Gradient Norm: 4.4334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:45<21:28,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 7, Average Validation Loss: 1.0927\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:52<21:24,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 8, Average Training Loss: 1.6680, Average Gradient Norm: 3.7714\n",
      "End of Epoch 8, Average Validation Loss: 1.1447\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [00:56<17:59,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 9, Average Training Loss: 1.6489, Average Gradient Norm: 4.0260\n",
      "End of Epoch 9, Average Validation Loss: 1.2113\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [00:59<15:42,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 10, Average Training Loss: 1.7494, Average Gradient Norm: 4.6954\n",
      "End of Epoch 10, Average Validation Loss: 1.4450\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [01:02<13:56,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 11, Average Training Loss: 1.6944, Average Gradient Norm: 4.7281\n",
      "End of Epoch 11, Average Validation Loss: 1.4728\n",
      "Current Learning Rate: 0.010000\n",
      "End of Epoch 12, Average Training Loss: 1.7007, Average Gradient Norm: 4.7559\n",
      "End of Epoch 12, Average Validation Loss: 1.1778\n",
      "Current Learning Rate: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/200 [01:09<11:58,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 13, Average Training Loss: 1.6421, Average Gradient Norm: 4.3769\n",
      "End of Epoch 13, Average Validation Loss: 1.1872\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15/200 [01:12<11:12,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 14, Average Training Loss: 1.5453, Average Gradient Norm: 3.8904\n",
      "End of Epoch 14, Average Validation Loss: 1.0761\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 15, Average Training Loss: 1.5142, Average Gradient Norm: 4.5262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [01:15<10:58,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 15, Average Validation Loss: 1.0651\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 17/200 [01:19<10:45,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 16, Average Training Loss: 1.5104, Average Gradient Norm: 4.9723\n",
      "End of Epoch 16, Average Validation Loss: 1.1048\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 17, Average Training Loss: 1.4857, Average Gradient Norm: 5.3314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [01:22<10:45,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 17, Average Validation Loss: 1.0553\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [01:26<10:41,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 18, Average Training Loss: 1.4993, Average Gradient Norm: 5.5465\n",
      "End of Epoch 18, Average Validation Loss: 1.0713\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [01:30<10:40,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 19, Average Training Loss: 1.4755, Average Gradient Norm: 5.4476\n",
      "End of Epoch 19, Average Validation Loss: 1.0705\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [01:33<10:31,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 20, Average Training Loss: 1.4794, Average Gradient Norm: 5.5877\n",
      "End of Epoch 20, Average Validation Loss: 1.1390\n",
      "Current Learning Rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 22/200 [01:36<10:04,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 21, Average Training Loss: 1.4720, Average Gradient Norm: 5.8628\n",
      "End of Epoch 21, Average Validation Loss: 1.0864\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 22, Average Training Loss: 1.4580, Average Gradient Norm: 5.8257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [01:41<11:33,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 22, Average Validation Loss: 1.0851\n",
      "Current Learning Rate: 0.001000\n",
      "End of Epoch 23, Average Training Loss: 1.4584, Average Gradient Norm: 6.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 24/200 [01:48<14:14,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 23, Average Validation Loss: 1.1197\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 24, Average Training Loss: 1.4553, Average Gradient Norm: 5.8099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 25/200 [01:54<15:09,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 24, Average Validation Loss: 1.0749\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 25, Average Training Loss: 1.4323, Average Gradient Norm: 5.3524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 26/200 [01:58<13:44,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 25, Average Validation Loss: 1.0751\n",
      "Current Learning Rate: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 27/200 [02:01<12:29,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 26, Average Training Loss: 1.4338, Average Gradient Norm: 5.8013\n",
      "End of Epoch 26, Average Validation Loss: 1.0671\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 27, Average Training Loss: 1.4328, Average Gradient Norm: 5.5567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [02:07<13:17,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 27, Average Validation Loss: 1.0824\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 28, Average Training Loss: 1.4460, Average Gradient Norm: 5.9432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 29/200 [02:14<15:35,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 28, Average Validation Loss: 1.0674\n",
      "Current Learning Rate: 0.000100\n",
      "End of Epoch 29, Average Training Loss: 1.4268, Average Gradient Norm: 5.7717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30/200 [02:21<16:51,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 29, Average Validation Loss: 1.0653\n",
      "Current Learning Rate: 0.000010\n",
      "End of Epoch 30, Average Training Loss: 1.4347, Average Gradient Norm: 5.6468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 31/200 [02:28<17:35,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 30, Average Validation Loss: 1.0656\n",
      "Current Learning Rate: 0.000010\n",
      "End of Epoch 31, Average Training Loss: 1.4469, Average Gradient Norm: 6.0499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 32/200 [02:35<17:55,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 31, Average Validation Loss: 1.0672\n",
      "Current Learning Rate: 0.000010\n",
      "End of Epoch 32, Average Training Loss: 1.4339, Average Gradient Norm: 5.6660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 33/200 [02:42<18:13,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 32, Average Validation Loss: 1.0681\n",
      "Current Learning Rate: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 34/200 [02:48<17:46,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 33, Average Training Loss: 1.4451, Average Gradient Norm: 5.6683\n",
      "End of Epoch 33, Average Validation Loss: 1.0670\n",
      "Current Learning Rate: 0.000010\n",
      "End of Epoch 34, Average Training Loss: 1.4427, Average Gradient Norm: 5.8478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 35/200 [02:51<15:16,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 34, Average Validation Loss: 1.0656\n",
      "Current Learning Rate: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 36/200 [02:55<13:29,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 35, Average Training Loss: 1.4503, Average Gradient Norm: 5.8291\n",
      "End of Epoch 35, Average Validation Loss: 1.0649\n",
      "Current Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 37/200 [02:58<11:56,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 36, Average Training Loss: 1.4278, Average Gradient Norm: 5.5817\n",
      "End of Epoch 36, Average Validation Loss: 1.0649\n",
      "Current Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 38/200 [03:01<10:58,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 37, Average Training Loss: 1.4584, Average Gradient Norm: 6.0866\n",
      "End of Epoch 37, Average Validation Loss: 1.0651\n",
      "Current Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 39/200 [03:05<10:16,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 38, Average Training Loss: 1.4437, Average Gradient Norm: 5.6536\n",
      "End of Epoch 38, Average Validation Loss: 1.0652\n",
      "Current Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [03:08<09:51,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 39, Average Training Loss: 1.4543, Average Gradient Norm: 6.2975\n",
      "End of Epoch 39, Average Validation Loss: 1.0652\n",
      "Current Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [03:11<09:24,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 40, Average Training Loss: 1.4299, Average Gradient Norm: 5.8032\n",
      "End of Epoch 40, Average Validation Loss: 1.0652\n",
      "Current Learning Rate: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 42/200 [03:15<09:10,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 41, Average Training Loss: 1.4518, Average Gradient Norm: 5.7269\n",
      "End of Epoch 41, Average Validation Loss: 1.0653\n",
      "Current Learning Rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 42/200 [03:18<12:27,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 42, Average Training Loss: 1.4390, Average Gradient Norm: 6.1429\n",
      "End of Epoch 42, Average Validation Loss: 1.0653\n",
      "Current Learning Rate: 0.000000\n",
      "Early stopping triggered after 43 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from expected_gradient_multi_task import AttributionPriorExplainer\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "# Define device and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiTaskModel(in_features, out_features).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "lambda_reg = 0.01\n",
    "APExp = AttributionPriorExplainer(train_dataset,args.batch_size)\n",
    "\n",
    "\n",
    "# Early stopping and checkpointing variables\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop_patience = 25\n",
    "checkpoint_path = 'outputfiles/best_model.pth'\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='outputfiles/checkpoint.pth'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        torch.save(state, checkpoint_path)\n",
    "\n",
    "def compute_gradients(features, task_output):\n",
    "        inp_grad = grad(outputs=torch.log(task_output),\n",
    "                        inputs=features,\n",
    "                        grad_outputs=torch.ones_like(task_output),\n",
    "                        create_graph=True)[0]\n",
    "        return inp_grad        \n",
    "\n",
    "for epoch in trange(args.epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    gradient_norms = []\n",
    "    for batch_idx, (X_train, targets, masks, event_train) in enumerate(train_loader):\n",
    "        \n",
    "        X_train = X_train.to(device)\n",
    "        X_train.requires_grad = True\n",
    "        targets = [target.to(device) for target in targets]\n",
    "        masks = [mask.to(device) for mask in masks]\n",
    "        optimizer.zero_grad()\n",
    "        task_outputs = model(X_train)\n",
    "        \n",
    "        ## get Expected Gradients attribution\n",
    "        eg = APExp.shap_values(model,X_train)\n",
    "\n",
    "        # Base loss computation\n",
    "        L_base = model.custom_loss(task_outputs, targets, masks)\n",
    "\n",
    "        Omega_smooth = 0\n",
    "        prev_attributions = None\n",
    "        for t in range(out_features):\n",
    "            current_attributions = eg[t] #compute_gradients(X_train, task_outputs[t])#eg[t] # compute_gradients(X_train, task_outputs[t])\n",
    "            # Compute smoothing regularization\n",
    "            if prev_attributions is not None:\n",
    "                diff = current_attributions - prev_attributions\n",
    "                Omega_smooth += torch.abs(diff)              \n",
    "            prev_attributions = current_attributions\n",
    "\n",
    "        # The formula in project description is summation, but I got better result by taking average.\n",
    "        Omega_smooth = torch.sum(Omega_smooth)  # Omega_smooth.mean() #torch.sum(Omega_smooth) \n",
    "        loss = L_base + (lambda_reg * Omega_smooth)\n",
    "        loss.backward(retain_graph=True)\n",
    "         # Compute and store gradient norms\n",
    "        total_norm = 0\n",
    "        for param in model.parameters():\n",
    "            param_norm = param.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        gradient_norms.append(total_norm)\n",
    "        \n",
    "        \n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.clip)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_grad_norm = sum(gradient_norms) / len(gradient_norms)\n",
    "    # print(f'End of Epoch {epoch}, Average Training Loss: {avg_loss:.4f}')\n",
    "    print(f'End of Epoch {epoch}, Average Training Loss: {avg_loss:.4f}, Average Gradient Norm: {avg_grad_norm:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, val_targets, val_masks, event_val in test_loader:\n",
    "            X_val = X_val.to(device)\n",
    "            val_targets = [target.to(device) for target in val_targets]\n",
    "            val_masks = [mask.to(device) for mask in val_masks]\n",
    "            val_outputs = model(X_val)\n",
    "            loss = model.custom_loss(val_outputs, val_targets, val_masks)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    print(f'End of Epoch {epoch}, Average Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Update the learning rate based on the validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Print the current learning rate\n",
    "    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    print(f'Current Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "    # Check if this is the best validation loss we've seen so far\n",
    "    is_best = avg_val_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs.')\n",
    "        break\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:48:42.433591800Z",
     "start_time": "2024-06-20T23:45:22.919134600Z"
    }
   },
   "id": "cf878a9c2410df02"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:01:18.546850300Z",
     "start_time": "2024-06-20T23:01:18.501850700Z"
    }
   },
   "id": "864a7474291c20"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint at epoch 18 with best validation loss 1.0553\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiTaskModel(in_features, out_features).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('outputfiles/best_model.pth')\n",
    "\n",
    "# Restore the model and optimizer state\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Optionally, restore other variables such as epoch and best validation loss\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "print(f\"Loaded model from checkpoint at epoch {start_epoch} with best validation loss {best_val_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:48:42.447591700Z",
     "start_time": "2024-06-20T23:48:42.424592900Z"
    }
   },
   "id": "25330793ed14a6a5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 7.], device='cuda:0')\n",
      "7\n",
      "torch.Size([3648, 1])\n",
      "torch.Size([3648])\n",
      "tensor([0., 1., 2., 3., 4., 7.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3648/3648 [00:01<00:00, 2381.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5437393, device='cuda:0')\n",
      "tensor(5569534, device='cuda:0')\n",
      "0.976274311542511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Cindex(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cindex, self).__init__()\n",
    "\n",
    "    def forward(self, y, y_hat, status):\n",
    "        if not torch.is_tensor(y):\n",
    "            y = torch.Tensor(y).to(\"cuda\")\n",
    "        if not torch.is_tensor(y_hat):\n",
    "            y_hat = torch.Tensor(y_hat).to(\"cuda\")\n",
    "        if not torch.is_tensor(status):\n",
    "          status = torch.Tensor(status).to(\"cuda\")\n",
    "            \n",
    "        N = y.size(0)\n",
    "        total_pairs = 0\n",
    "        c = 0\n",
    "        \n",
    "        for i in trange(N):\n",
    "            a = y[i]\n",
    "            a_hat = y_hat[i]\n",
    "            b = y[i+1:]\n",
    "            b_hat = y_hat[i+1:]\n",
    "            astatus = status[i]\n",
    "            bstatus = status[i+1:]\n",
    "            \n",
    "            a_greater = a >= b\n",
    "            a_hat_greater = a_hat >= b_hat\n",
    "            bstatus_1 = bstatus == 1\n",
    "            \n",
    "            a_lesser =  a <= b\n",
    "            a_hat_lesser = a_hat <= b_hat\n",
    "            astatus_1 = astatus == 1\n",
    "            c1 = torch.logical_and(a_greater.to(\"cuda\"), a_hat_greater.to(\"cuda\"))\n",
    "            c1 = torch.logical_and(c1, bstatus_1.to(\"cuda\"))\n",
    "            \n",
    "            c2 = torch.logical_and(a_lesser.to(\"cuda\"), a_hat_lesser.to(\"cuda\"))\n",
    "            c2 = torch.logical_and(c2, astatus_1.to(\"cuda\"))\n",
    "            cc = torch.logical_or(c1, c2)\n",
    "            c += torch.sum(cc.long())\n",
    "            \n",
    "            tp1 = torch.logical_and(a_lesser.to(\"cuda\"), astatus_1.to(\"cuda\"))\n",
    "            tp2 = torch.logical_and(a_greater.to(\"cuda\"), bstatus_1.to(\"cuda\"))\n",
    "            tp = torch.logical_or(tp1, tp2)\n",
    "            total_pairs += torch.sum(tp.long())\n",
    "            \n",
    "        \n",
    "        # for i in trange(N):\n",
    "        #     for j in range(i + 1, N):\n",
    "        #         a = y[i]\n",
    "        #         b = y[j]\n",
    "        #         a_hat = y_hat[i]\n",
    "        #         b_hat = y_hat[j]\n",
    "        #         astatus = status[i]\n",
    "        #         bstatus = status[j]\n",
    "        #         if (a >= b and a_hat >= b_hat and bstatus == 1) or (a <= b and a_hat <= b_hat and astatus == 1):\n",
    "        #             c += 1\n",
    "        #         if (a <= b and astatus==1) or (b <= a and bstatus == 1):\n",
    "        #             total_pairs += 1\n",
    "        \n",
    "        print(c)\n",
    "        print(total_pairs)\n",
    "        outcome = c / total_pairs\n",
    "        return outcome.cpu().item()\n",
    "\n",
    "\n",
    "# made by Xinyu\n",
    "def binarize_and_sum_columns(output_list):\n",
    "    def binarize_list(input_list):\n",
    "        tensor = torch.Tensor(input_list)\n",
    "        # print(input_list.max() == input_list.min())\n",
    "        binary_tensor = (tensor >= 0.5).float()\n",
    "        return binary_tensor\n",
    "\n",
    "    result = binarize_list(output_list[0])\n",
    "    for i in range(1, len(output_list)):\n",
    "        binary_column = binarize_list(output_list[i])\n",
    "        # print(binary_column.max() == binary_column.min())\n",
    "        result += binary_column\n",
    "\n",
    "    return result\n",
    "\n",
    "# Created by Jingyan\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "# Disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    for X_test, _, _, _ in test_loader:\n",
    "        # Forward pass\n",
    "        X_test = X_test.to(device)\n",
    "        task_outputs_ = model(X_test)\n",
    "\n",
    "        # Store the predictions\n",
    "        test_predictions.append(task_outputs_)\n",
    "test_predictions = [torch.cat([preds[i] for preds in test_predictions]) for i in range(len(test_predictions[0]))]\n",
    "Y_hat_test = binarize_and_sum_columns(test_predictions)\n",
    "Y_hat_test = Y_hat_test.squeeze()\n",
    "print(Y_hat_test.unique())\n",
    "\n",
    "\n",
    "\n",
    "test_trues = []\n",
    "test_statuses = []\n",
    "for _, test_targets, test_masks, test_status in test_loader:\n",
    "    true_label = [test_targets[i]*test_masks[i] for i in range(len(test_targets))]\n",
    "    test_trues.append(true_label)\n",
    "    test_statuses.append(test_status)\n",
    "\n",
    "test_trues = [torch.cat([preds[i] for preds in test_trues]) for i in range(len(test_trues[0]))]\n",
    "test_statuses = torch.cat([status for status in test_statuses])\n",
    "\n",
    "print(len(test_trues))\n",
    "print(test_trues[0].shape)\n",
    "print(test_statuses.shape)\n",
    "\n",
    "Y_true_test = binarize_and_sum_columns(test_trues)\n",
    "Y_true_test = Y_true_test.squeeze()\n",
    "print(Y_hat_test.unique())\n",
    "cindex_calculator = Cindex()\n",
    "c11_test = cindex_calculator(Y_true_test, Y_hat_test, test_statuses)\n",
    "print(c11_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:48:44.217592200Z",
     "start_time": "2024-06-20T23:48:42.444592100Z"
    }
   },
   "id": "a9f472f4054dfb97"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0.06945544592747543"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sksurv.metrics import integrated_brier_score\n",
    "\n",
    "dtype = [('cens', bool), ('time', int)]\n",
    "# Create the structured array\n",
    "all_array = np.array(list(zip(event_all.astype(bool), Y.sum(axis=1).cpu())), dtype=dtype)\n",
    "test_array = np.array(list(zip(test_statuses.cpu().numpy().astype(bool), Y_true_test.cpu())), dtype=dtype)\n",
    "_times = np.arange(1, 7)\n",
    "probs = np.column_stack([item.cpu().numpy() for item in test_predictions])\n",
    "integrated_brier_score(all_array, test_array, probs[:, 1:7], _times)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:48:44.284592100Z",
     "start_time": "2024-06-20T23:48:44.205591900Z"
    }
   },
   "id": "6dfc08e8f60b3f92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 0.9685770869255066 0.9811781644821167, 0.06966835282499209 0.07313522417625694\n",
    "# 0.9815456867218018, 0.13459566972554354\n",
    "# 0.9756746292114258, 0.06753334177025772"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df3203bf5efc61dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# C-index\n",
    "\n",
    "# batch 64 and lambda 0.1 = 0.9747766101661264\n",
    "# batch 64 and lambda 0.01 = 0.9727702104237926\n",
    "# batch 64 and lambda 0.001 = 0.9609226584434509\n",
    "# batch 32 and lambda 0.001 = 0.9801711440086365\n",
    "# Default batch 64 = 0.980457603931427\n",
    "\n",
    "\n",
    "# batch 64 and lambda 0.001 = 0.9748636484146118 # method EG  0.9782829284667969\n",
    "# batch 64 and lambda 0.01 = 0.9813091158866882 # method gradient\n",
    "# batch 64  = 0.9793468713760376  # method No regularization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36293a0b97aff339"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IB Score\n",
    "\n",
    "# batch 64 and lambda 0.001 = 0.06530525413905933\n",
    "# Default batch 64 = 0.06640703696079571\n",
    "\n",
    "\n",
    "# batch 64 and lambda 0.01 = 0.06833509861869876 # method EG  0.06989871045566962\n",
    "# batch 64 and lambda 0.01 = 0.13501370015201017 # method gradient\n",
    "# batch 64 0.06639405091289838 # method No regularization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebe474c24098156a"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.85      0.92      1026\n",
      "         1.0       0.81      0.63      0.71      1009\n",
      "         2.0       0.28      0.36      0.31       404\n",
      "         3.0       0.19      0.11      0.14       507\n",
      "         4.0       0.25      0.09      0.13       406\n",
      "         5.0       0.00      0.00      0.00       214\n",
      "         6.0       0.00      0.00      0.00        64\n",
      "         7.0       0.02      0.89      0.03        18\n",
      "\n",
      "    accuracy                           0.48      3648\n",
      "   macro avg       0.32      0.37      0.28      3648\n",
      "weighted avg       0.59      0.48      0.52      3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atlas\\anaconda3\\envs\\future\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Atlas\\anaconda3\\envs\\future\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Atlas\\anaconda3\\envs\\future\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Default model\")\n",
    "print(classification_report(Y_true_test.cpu(), Y_hat_test.cpu()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:01:59.719078500Z",
     "start_time": "2024-06-20T23:01:59.680078900Z"
    }
   },
   "id": "97d2456e925130f2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.85      0.91      1026\n",
      "         1.0       0.73      0.70      0.71      1009\n",
      "         2.0       0.26      0.17      0.20       404\n",
      "         3.0       0.24      0.19      0.21       507\n",
      "         4.0       0.60      0.01      0.01       406\n",
      "         5.0       0.00      0.00      0.00       214\n",
      "         6.0       0.00      0.00      0.00        64\n",
      "         7.0       0.01      0.89      0.03        18\n",
      "\n",
      "    accuracy                           0.48      3648\n",
      "   macro avg       0.35      0.35      0.26      3648\n",
      "weighted avg       0.60      0.48      0.51      3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atlas\\anaconda3\\envs\\future\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Atlas\\anaconda3\\envs\\future\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Atlas\\anaconda3\\envs\\future\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Reg model\")\n",
    "print(classification_report(Y_true_test.cpu(), Y_hat_test.cpu()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:48:52.081615800Z",
     "start_time": "2024-06-20T23:48:52.053616Z"
    }
   },
   "id": "f6114d130e2070"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 0., 4.,  ..., 0., 2., 5.])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_true_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:10:46.545724100Z",
     "start_time": "2024-06-20T23:10:46.514724500Z"
    }
   },
   "id": "8728bfa6e32bf69e"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 0., 7.,  ..., 0., 1., 7.], device='cuda:0')"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:10:47.327265500Z",
     "start_time": "2024-06-20T23:10:47.303266300Z"
    }
   },
   "id": "45207eaaecc82f9f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "torch.Size([11008, 1])\n",
      "torch.Size([11008])\n",
      "torch.Size([11008])\n"
     ]
    }
   ],
   "source": [
    "train_trues = []\n",
    "train_statuses = []\n",
    "for _, train_targets, train_masks, train_status in train_loader:\n",
    "    true_label = [train_targets[i]*train_masks[i] for i in range(len(train_targets))]\n",
    "    train_trues.append(true_label)\n",
    "    train_statuses.append(train_status)\n",
    "train_trues = [torch.cat([preds[i] for preds in train_trues]) for i in range(len(train_trues[0]))]\n",
    "train_statuses = torch.cat([status for status in train_statuses])\n",
    "\n",
    "print(len(train_trues))\n",
    "print(train_trues[0].shape)\n",
    "print(train_statuses.shape)\n",
    "\n",
    "Y_true_train = binarize_and_sum_columns(train_trues)\n",
    "Y_true_train = Y_true_train.squeeze()\n",
    "print(Y_true_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:49:55.613714Z",
     "start_time": "2024-06-20T23:49:55.330713100Z"
    }
   },
   "id": "688a52d244f7e997"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: tensor([0., 1., 2., 3., 4., 5., 6., 7.])\n",
      "Counts: tensor([3065, 3019, 1206, 1598, 1211,  675,  191,   43])\n"
     ]
    }
   ],
   "source": [
    "def unique_value_counts(tensor):\n",
    "    # Convert the PyTorch tensor to a NumPy array\n",
    "    np_array = tensor.cpu().numpy()\n",
    "    \n",
    "    # Use np.unique to get unique values and their counts\n",
    "    unique_values, counts = np.unique(np_array, return_counts=True)\n",
    "    \n",
    "    # Convert the results back to PyTorch tensors (if needed)\n",
    "    unique_values_tensor = torch.from_numpy(unique_values)\n",
    "    counts_tensor = torch.from_numpy(counts)\n",
    "    \n",
    "    return unique_values_tensor, counts_tensor\n",
    "\n",
    "# Example usage\n",
    "unique_values, counts = unique_value_counts(Y_true_train)\n",
    "\n",
    "print(\"Unique Values:\", unique_values)\n",
    "print(\"Counts:\", counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:49:55.885712600Z",
     "start_time": "2024-06-20T23:49:55.866712400Z"
    }
   },
   "id": "8fab8f91ccc33716"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: tensor([0., 1., 2., 3., 4., 5., 6., 7.])\n",
      "Counts: tensor([1026, 1009,  404,  507,  406,  214,   64,   18])\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = unique_value_counts(Y_true_test)\n",
    "\n",
    "print(\"Unique Values:\", unique_values)\n",
    "print(\"Counts:\", counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:11:23.125490Z",
     "start_time": "2024-06-20T23:11:23.098487Z"
    }
   },
   "id": "3c06c6eec06b8724"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: tensor([0., 1., 2., 3., 4., 5., 7.])\n",
      "Counts: tensor([ 874,  778,  527,  283,  142,    4, 1040])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_values, counts = unique_value_counts(Y_hat_test)\n",
    "\n",
    "print(\"Unique Values:\", unique_values)\n",
    "print(\"Counts:\", counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:03:23.623304600Z",
     "start_time": "2024-06-20T23:03:23.598304Z"
    }
   },
   "id": "3ed323e4b3d8feca"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values: tensor([0., 1., 2., 3., 4., 7.])\n",
      "Counts: tensor([ 903,  963,  255,  401,    5, 1121])\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = unique_value_counts(Y_hat_test)\n",
    "\n",
    "print(\"Unique Values:\", unique_values)\n",
    "print(\"Counts:\", counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T23:49:58.240825300Z",
     "start_time": "2024-06-20T23:49:58.197825300Z"
    }
   },
   "id": "678210f8e7409330"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T18:10:07.488368400Z",
     "start_time": "2024-06-20T18:10:07.467367100Z"
    }
   },
   "id": "ec16770a114178f5"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T18:10:58.059834900Z",
     "start_time": "2024-06-20T18:10:58.015835300Z"
    }
   },
   "id": "d88d25fe11035345"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0.06640703696079571"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T18:11:30.220658100Z",
     "start_time": "2024-06-20T18:11:30.193658700Z"
    }
   },
   "id": "f94cadc247cb3bfd"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array['time'].min()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T18:07:30.047940700Z",
     "start_time": "2024-06-20T18:07:30.006942800Z"
    }
   },
   "id": "ca96b4d8fe8351cd"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n       [9.99568641e-01, 4.11797166e-01, 1.86812818e-01, ...,\n        8.14564526e-03, 8.70392032e-05, 1.07667552e-06],\n       [9.99903202e-01, 1.09691195e-01, 4.27960083e-02, ...,\n        4.37506149e-03, 2.17447068e-05, 7.80379014e-06],\n       ...,\n       [9.99997616e-01, 9.91505980e-01, 9.46225166e-01, ...,\n        4.11568075e-01, 5.07023670e-02, 1.37454060e-06],\n       [9.99976754e-01, 7.93147922e-01, 3.54994208e-01, ...,\n        1.37929770e-03, 1.00549541e-05, 1.67736189e-08],\n       [4.05517123e-17, 2.13318832e-27, 2.10200944e-27, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]], dtype=float32)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T18:03:03.286187300Z",
     "start_time": "2024-06-20T18:03:03.216187900Z"
    }
   },
   "id": "443d751ee76b6c0"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0000],\n        [0.9996],\n        [0.9999],\n        [0.9992],\n        [0.9994]], device='cuda:0')"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[0][:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T18:03:15.835997100Z",
     "start_time": "2024-06-20T18:03:15.788997800Z"
    }
   },
   "id": "647ed7cb31a74ca2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "13839c2770000bb5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
